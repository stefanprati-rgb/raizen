Arquitetura Avançada de Extração de Múltiplas Entidades em Contratos de Energia: Uma Abordagem Híbrida OfflineSumário ExecutivoO presente relatório técnico delineia uma estratégia exaustiva para a resolução de um problema de alta complexidade no domínio da Inteligência Documental (Document Intelligence): a extração de múltiplos identificadores de "Unidades Consumidoras" (UCs) a partir de um corpus heterogêneo de aproximadamente 2.200 contratos de energia das distribuidoras CPFL Paulista e Raízen Power. O desafio central transcende a simples extração de texto, situando-se na necessidade de identificar, isolar e validar relações de entidade "um-para-muitos" (1:N) em documentos não estruturados e semiestruturados, operando sob restrições estritas de processamento offline (Python) e exigindo precisão superior a 95%.A análise aprofundada das tecnologias disponíveis e das características intrínsecas dos documentos alvo aponta para a insuficiência de abordagens monolíticas. A utilização isolada de Expressões Regulares (Regex) falha diante da não-linearidade das tabelas em PDF, enquanto modelos generativos (como o Gemini, mencionado na stack atual) violam o requisito de operação offline e introduzem latência proibitiva. A "clusterização" algorítmica pura, embora útil, carece da compreensão semântica necessária para distinguir rótulos de valores em layouts complexos.Consequentemente, este documento propõe a implementação de um Pipeline Híbrido Baseado em Roteamento (Router Pattern). Esta arquitetura dinâmica classifica cada página do documento para direcioná-la a motores de extração especializados: GMFT (Geometric Model-Free Tracking/Table Transformer) para dados tabulares complexos, Regex Espacial (Spatial Regex) com PyMuPDF para listas não estruturadas, e uma camada de contingência baseada em OCR Tesseract/DocTR para digitalizações rasterizadas. A validação dos dados é garantida não apenas por heurísticas, mas pela implementação algorítmica dos dígitos verificadores (Módulo 11) específicos do padrão CPFL, assegurando a fidelidade dos dados críticos para o faturamento e gestão de energia.1. Engenharia de Domínio e Análise do Problema1.1 A Natureza Crítica da Unidade Consumidora (UC)No ecossistema do setor elétrico brasileiro, especificamente no contexto de Geração Distribuída (GD) e Mercado Livre de Energia (ACL), a Unidade Consumidora (UC) é a entidade atômica fundamental. Diferente de um contrato de fornecimento residencial padrão, que possui uma relação 1:1 (um contrato, uma UC), os documentos corporativos da Raízen Power e CPFL Paulista frequentemente estabelecem relações 1:N. Um único "Termo de Adesão" ou "Aditivo de Geração Compartilhada" pode vincular uma usina geradora a dezenas ou centenas de UCs beneficiárias.1A extração precisa desses identificadores é, portanto, uma tarefa de alto risco. A falha na captura de uma única UC em uma lista de cinquenta pode resultar em erros de faturamento recorrentes e inconformidade regulatória. O formato da UC na área de concessão da CPFL Paulista segue um padrão numérico rigoroso. Historicamente, e conforme regulamentações da ANEEL, este identificador é composto por uma sequência de dígitos (frequentemente entre 8 e 12 dígitos), que pode ou não ser apresentada com um dígito verificador separado por hífen (ex: 12345678-9).2A variabilidade deste formato nos documentos é um dos vetores de complexidade. Em faturas antigas ou contratos legados digitalizados, a UC pode aparecer apenas como um bloco numérico sem formatação. Em contratos digitais modernos ("nativos"), ela pode estar formatada, mas inserida em células de tabelas complexas que desafiam a extração linear de texto. A presença de outros identificadores numéricos com morfologia similar — como números de medidores, códigos de instalação, CNPJs truncados ou protocolos de atendimento — exige que o sistema de extração possua uma "consciência" contextual aguçada para diferenciar o sinal (UC) do ruído (outros números).31.2 Taxonomia dos Documentos e Desafios de LayoutO corpus de ~2.200 documentos não é uniforme. Ele é composto por uma amálgama de "Termos de Adesão", "Contratos Solar", "Aditivos Contratuais" e "Memórias de Cálculo". Cada tipologia documental apresenta as UCs de maneira distinta, exigindo estratégias de extração adaptativas.1.2.1 Tabelas Estruturadas e Bordas InvisíveisA forma mais comum de apresentação de múltiplas UCs é tabular. No entanto, em PDFs, uma "tabela" é uma construção visual, não lógica. O arquivo PDF armazena instruções de desenho de linhas e posicionamento de caracteres, mas não possui a tag semanticamente rica <table> encontrada em HTML.O Desafio da Linearização: Ferramentas básicas de extração de texto (como page.get_text() simples do PyMuPDF) leem o arquivo da esquerda para a direita, de cima para baixo. Em uma tabela de duas colunas onde a Coluna A contém a UC e a Coluna B contém o Endereço, a linearização pode resultar em UC1 Endereço1 UC2 Endereço2 ou, pior, misturar linhas se o espaçamento for irregular.Tabelas sem Bordas (Borderless): Muitos aditivos contratuais utilizam tabelas "invisíveis", onde o alinhamento visual sugere colunas, mas não existem linhas vetoriais demarcando as células. Algoritmos baseados em detecção de linhas (como o método Lattice do Camelot) falham catastroficamente nestes casos.51.2.2 Listas em Texto Corrido (Unstructured Lists)Em "Termos de Adesão" menos formais ou corpos de e-mail anexados ao PDF, as UCs podem ser listadas em parágrafos de texto corrido: "O presente aditivo inclui as unidades 123456, 789012 e 345678 ao contrato original...".O Desafio da Ambiguidade: Aqui, a estrutura tabular inexiste. A extração depende inteiramente de âncoras textuais (keywords) e proximidade espacial. Regex simples pode capturar os números, mas sem a validação contextual de que esses números são, de fato, UCs e não valores monetários ou datas, a precisão cai drasticamente.71.2.3 O Formato Misto (Híbrido)A coexistência de documentos nativos digitais e digitalizações (scans) é o maior obstáculo para uma solução única.Nativos Digitais: Possuem camada de texto selecionável. São processáveis em milissegundos via PyMuPDF.Scans (Raster): São imagens dentro de um container PDF. Exigem OCR (Optical Character Recognition), que é computacionalmente custoso (segundos por página) e propenso a erros de reconhecimento (alucinação de caracteres, confusão entre 1, I e l).8A arquitetura proposta deve, obrigatoriamente, discernir entre estes formatos automaticamente para aplicar o método correto, otimizando o tempo de processamento e a precisão.2. Estratégia Arquitetural: O Padrão de Roteamento (Router Pipeline)Para atingir a meta de precisão >95% em um ambiente offline, rejeita-se a abordagem de "bala de prata" (um único modelo para tudo). Propõe-se uma arquitetura modular baseada no padrão de projeto Pipeline Router.9 Este design permite que o sistema inspecione cada página individualmente e decida a rota de processamento ideal, equilibrando custo computacional e especificidade.2.1 Componentes do PipelineA tabela abaixo resume a arquitetura proposta, substituindo componentes genéricos da stack atual por ferramentas especializadas identificadas na pesquisa.ComponenteStack Atual (Diagnóstico)Stack Proposta (Solução)Justificativa TécnicaIngestãoPython os/globpathlib + Hashing (SHA256)Detecção de duplicatas e manuseio robusto de caminhos.ClassificaçãoNão especificadoPyMuPDF + TF-IDF (Scikit-Learn)Distinção entre Texto/Imagem e filtragem de páginas relevantes.Extração TabularClusterização / RegexGMFT (Table Transformer)SOTA (State-of-the-Art) para tabelas complexas em CPUs offline.11Extração TextualRegex simplesPyMuPDF Spatial RegexRegex contextualizado por coordenadas (Bounding Boxes).12OCR (Legado)Não especificadoTesseract 5 / DocTRFallback robusto para documentos digitalizados.ValidaçãoNão especificadoAlgoritmo Módulo 11 (CPFL)Validação matemática determinística dos dígitos verificadores.132.2 Fluxo de Processamento LógicoO fluxo de dados segue uma lógica condicional estrita:Entrada: Arquivo PDF.Verificação de Camada de Texto: O arquivo possui texto selecionável?Não: Rota de Pré-processamento OCR (Geração de Camada de Texto Oculta).Sim: Segue para classificação.Classificação de Página: A página contém palavras-chave de interesse ("Unidade", "Tabela", "Rol", "Anexo")?Não: Descarte (Skip). Otimiza o tempo total ignorando cláusulas jurídicas irrelevantes.Sim: Identifica a estrutura predominante (Tabela ou Texto).Roteamento de Extração:Se Tabela: Envia para o motor GMFT.Se Lista: Envia para o motor Spatial Regex.Pós-Processamento: Normalização dos dados e validação via dígito verificador.3. O Motor de Extração Tabular: GMFT e Table TransformersO usuário mencionou "Clusterização" como parte da stack atual. Embora algoritmos de clusterização (como DBSCAN ou K-Means aplicados a coordenadas de texto) possam reconstruir tabelas simples, eles são frágeis diante de células mescladas, linhas múltiplas dentro de uma célula ou cabeçalhos complexos. A pesquisa aponta inequivocamente para o uso de Modelos de Visão Baseados em Transformadores como a solução superior para este problema.3.1 Por que GMFT (Geometric Model-Free Tracking)?O GMFT é uma biblioteca Python leve que atua como um wrapper (interface) para o modelo Table Transformer (TATR) da Microsoft.11Especialização: Ao contrário do LayoutLM, que é um modelo generalista de compreensão documental, o TATR foi treinado especificamente no dataset PubTables-1M para detectar e estruturar tabelas.15Independência de OCR: O GMFT brilha em PDFs nativos digitais. Ele utiliza as informações posicionais do texto já existentes no PDF, dispensando o uso de OCR pesado. Isso resulta em uma velocidade de inferência de ~1.3 segundos por página em CPU, sendo cerca de 10x mais rápido que pipelines que dependem de OCR para tudo (como Unstructured ou Nougat).11Offline por Design: O GMFT permite o carregamento de modelos pré-treinados a partir de diretórios locais, satisfazendo o requisito crítico de operação offline.173.2 Implementação Técnica Offline do GMFTPara operar offline, o modelo deve ser baixado previamente e transferido para o ambiente de execução.Passo 1: Download dos Pesos (Ambiente Online)Utiliza-se a função snapshot_download da biblioteca huggingface_hub para baixar os modelos de detecção e reconhecimento de estrutura.18Python# Script de preparação (Executar Online)
from huggingface_hub import snapshot_download
import os

# Definir diretório de cache local para transporte
local_cache = "./offline_models"
os.makedirs(local_cache, exist_ok=True)

# Baixar modelo de detecção de tabelas (TATR)
snapshot_download(repo_id="microsoft/table-transformer-detection", 
                  local_dir=f"{local_cache}/detection",
                  local_dir_use_symlinks=False)

# Baixar modelo de reconhecimento de estrutura
snapshot_download(repo_id="microsoft/table-transformer-structure-recognition", 
                  local_dir=f"{local_cache}/structure",
                  local_dir_use_symlinks=False)
Passo 2: Carregamento e Inferência (Ambiente Offline)No ambiente de produção desconectado, o GMFT é configurado para carregar estes modelos locais. O código abaixo ilustra a integração com o AutoModel do Transformers, essencial para contornar a tentativa de conexão com o Hub.17Python# Script de Produção (Offline)
from transformers import AutoModelForObjectDetection, TableTransformerForObjectDetection
from gmft.auto import AutoTableDetector, AutoTableFormatter

# Caminhos locais
det_path = "./offline_models/detection"
struc_path = "./offline_models/structure"

# Inicialização com caminhos locais
# O GMFT abstrai parte disso, mas para controle total offline, instanciamos os detectores
detector = AutoTableDetector(model_path=det_path) 
formatter = AutoTableFormatter(model_path=struc_path)

def extract_tables_from_page(page_image):
    # Extração de tabelas
    tables = detector.extract(page_image)
    dataframes =
    for table in tables:
        # Formatação para Pandas DataFrame
        df = formatter.extract(table).df()
        dataframes.append(df)
    return dataframes
3.3 Comparativo: GMFT vs. LayoutLM vs. DonutO usuário questionou sobre a escolha entre estes modelos. A análise comparativa justifica a escolha do GMFT para a tarefa de tabelas:CaracterísticaGMFT / TATRLayoutLMv3DonutAbordagemDetecção de Objetos (Tabelas)Compreensão Multimodal (Texto + Layout)OCR-Free (Imagem para Texto/JSON)Dependência de OCRNão (usa texto do PDF)Sim (requer OCR prévio para scans)Não (End-to-End)Precisão em TabelasExtrema (Especialista)Alta (se fine-tuned)Média (tende a alucinar em tabelas densas)Velocidade (CPU)~1.3s / página 11Lenta (pesada sem GPU)Lenta (Auto-regressivo)Necessidade de TreinoZero (Zero-shot no PubTables-1M)Alta (Fine-tuning exigido para bounding boxes)Alta (Fine-tuning exigido para estrutura JSON)Conclusão para CasoRecomendadoExagero (Overkill)Inadequado para precisão numérica estritaO Donut, embora promissor, é um modelo gerador de texto. Em contratos de energia onde um dígito errado na UC invalida a extração, o risco de "alucinação" (o modelo inventar um número plausível mas incorreto) é inaceitável. O GMFT, por ser extrativo (recorta e organiza o texto existente), preserva a integridade dos dados originais.214. O Motor de Extração de Listas: Spatial Regex com PyMuPDFPara as UCs que não estão em tabelas, a abordagem de "Spatial Regex" (Expressão Regular Espacial) é a evolução necessária da Regex simples.4.1 O Conceito de Ancoragem Visual (Visual Anchoring)Em vez de buscar um padrão numérico em todo o texto da página, o algoritmo primeiro localiza uma "âncora" semântica.Âncora: Termos como "Unidade Consumidora:", "Instalação:", "Referente à UC:".Bounding Box (BBox): Utiliza-se o método page.search_for("Unidade Consumidora") do PyMuPDF para obter as coordenadas (x0, y0, x1, y1) da âncora.224.2 Lógica de Varredura EspacialUma vez localizada a âncora, define-se uma Região de Interesse (ROI).Se a âncora é "Unidade Consumidora:", a ROI provável está imediatamente à direita (mesmo eixo Y, X maior) ou imediatamente abaixo (mesmo eixo X, Y maior).O algoritmo restringe a busca de texto (page.get_text("blocks", clip=ROI)) apenas a esta região.24Desta forma, uma Regex como \d{8,12} aplicada dentro da ROI tem uma probabilidade quase nula de capturar um CEP ou telefone que esteja no rodapé da página, pois estes estariam fora da caixa de recorte.4.3 Aprimorando a "Clusterização" ExistenteA stack atual do usuário menciona "Clusterização". Para listas não estruturadas, recomenda-se refinar essa técnica utilizando DBSCAN Unidimensional.Problema: Listas de UCs separadas por vírgulas ou quebras de linha irregulares.Solução: Extrair todas as palavras da ROI. Aplicar DBSCAN nas coordenadas Y. Palavras com Y muito próximos pertencem à mesma linha. Aplicar DBSCAN nas coordenadas X para identificar colunas implícitas. Isso permite reconstruir a ordem de leitura lógica mesmo em layouts visualmente quebrados, superando a linearização padrão do PDF.265. Camada de Validação e Qualidade de Dados (>95%)A extração mecânica de números é apenas metade da solução. Para garantir precisão >95%, o sistema deve rejeitar falsos positivos através de validação determinística.5.1 O Algoritmo do Dígito Verificador (Módulo 11)A CPFL, como muitas concessionárias, utiliza algoritmos de verificação para seus códigos de instalação. A validação matemática elimina "alucinações" de OCR e números aleatórios que coincidem com o padrão de comprimento da UC.Baseado na prática padrão do setor (análoga ao CPF/CNPJ) e na documentação regulatória 2, o pipeline deve implementar uma função Python de validação. Embora o algoritmo exato seja proprietário, ele geralmente segue o padrão Módulo 11:Pesos: Atribui-se pesos sequenciais aos dígitos da UC base.Soma Ponderada: Multiplica-se cada dígito pelo seu peso e soma-se os resultados.Resto: Calcula-se o resto da divisão da soma por 11.Dígito: Subtrai-se o resto de 11 (com regras especiais para restos 0 ou 1).A implementação de uma função validate_cpfl_uc(candidate_string) atua como o filtro final. Se o número extraído não satisfaz a condição matemática, ele é marcado como REVIEW_REQUIRED ou descartado, impedindo a poluição do banco de dados.5.2 Validação Contextual CruzadaAlém da matemática, o contexto valida a extração:Deduplicação: Uma UC frequentemente aparece no cabeçalho da página e na tabela anexa. Se o sistema extrai 12345678 da tabela e encontra o mesmo número no cabeçalho, a confiança na extração sobe para perto de 100%.Sanity Check de Formato: UCs da mesma distribuidora tendem a ter o mesmo número de dígitos. Se um contrato da CPFL Paulista lista 50 UCs, e 49 delas têm 10 dígitos, mas uma tem 9, a de 9 dígitos é estatisticamente uma falha de OCR (dígito perdido) e deve ser sinalizada.6. Operacionalização em Python Offline6.1 Tratamento de Digitalizações (Scans)Para os documentos classificados como imagem pelo Router, a solução offline exige Tesseract OCR ou DocTR.Otimização: Não aplique OCR no documento inteiro. Use o Router para identificar as páginas de anexo (geralmente no final) e aplique OCR apenas nelas.Pré-processamento: Utilize OpenCV para binarizar a imagem (preto e branco) e corrigir rotação (deskewing) antes do OCR. Isso é crítico para a precisão no reconhecimento de dígitos numéricos, onde um leve borrão pode transformar um 8 em um 3.276.2 Código de Exemplo: Estrutura do RouterPythonimport fitz  # PyMuPDF
from gmft.auto import AutoTableDetector, AutoTableFormatter
import re

def router_pipeline(pdf_path):
    doc = fitz.open(pdf_path)
    extracted_data =

    for page_num, page in enumerate(doc):
        # 1. Verificação de Texto (Scan vs Digital)
        text_content = page.get_text()
        if len(text_content) < 50:
            print(f"Página {page_num} é scan. Enviando para OCR...")
            # Chamar função de OCR aqui (ex: pytesseract)
            continue

        # 2. Classificação Semântica (A página tem UCs?)
        if not re.search(r"(unidade|instalação|código|anexo)", text_content, re.IGNORECASE):
            continue # Pular página irrelevante

        # 3. Decisão de Estrutura (Tabela vs Texto)
        # GMFT é barato, podemos tentar rodar em todas as páginas alvo
        tables = detector.extract(page) # Detector GMFT carregado offline
        
        if tables:
            # Rota de Tabela
            for table in tables:
                df = formatter.extract(table).df()
                ucs = extract_ucs_from_dataframe(df) # Lógica customizada de colunas
                extracted_data.extend(ucs)
        else:
            # Rota de Texto (Spatial Regex)
            ucs = extract_ucs_spatial_regex(page)
            extracted_data.extend(ucs)
            
    return list(set(extracted_data)) # Deduplicação
7. Análise Comparativa de AlternativasPara fundamentar a decisão arquitetural, apresenta-se uma comparação técnica final entre as opções consideradas:CritérioRegex PuroClusterização SimplesLayoutLM / DonutGMFT + Router (Proposto)Lidar com TabelasFraco (perde estrutura)Médio (falha em bordas)Forte (mas lento)Excelente (Especialista)Lidar com ScansImpossívelImpossívelLayoutLM sim (com OCR)Sim (via Router + OCR)OfflineSimSimSim (complexo config)Sim (nativo)VelocidadeInstantâneoMuito RápidoLento (>5s/pág)Rápido (~1.3s/pág)PrecisãoBaixa em layouts mistosMédiaAltaMuito Alta (>95%)8. ConclusãoO desafio de extrair múltiplas UCs de 2.200 contratos heterogêneos não pode ser resolvido por força bruta ou por um único algoritmo. A solução reside na inteligência do fluxo de trabalho. Ao adotar o GMFT como motor central para tabelas — aproveitando sua capacidade de operar offline e sua especialização em estruturas de grade — e complementá-lo com Regex Espacial para textos não estruturados, o pipeline proposto atende aos requisitos de precisão e operacionalidade.A substituição do modelo generativo (Gemini) por modelos determinísticos e extrativos (TATR/Regex) não apenas viabiliza a operação offline, mas também aumenta a auditabilidade e a confiabilidade dos dados extraídos, eliminando o risco de alucinações. Com a adição da camada de validação via Módulo 11, o sistema está arquitetado para superar a barreira dos 95% de precisão, transformando documentos passivos em dados estruturados acionáveis para a gestão de energia.9. Referências e Bibliografia Técnica CitadaA fundamentação técnica deste relatório baseia-se na análise de bibliotecas modernas de processamento de documentos e benchmarks de performance.Benchmarks de Extração de Tabelas em PDF: 5Capacidades do GMFT e Table Transformers: 11Configuração Offline de Modelos Transformers: 18Técnicas de PyMuPDF e Regex Espacial: 12Validação de Dígitos e UCs: 2